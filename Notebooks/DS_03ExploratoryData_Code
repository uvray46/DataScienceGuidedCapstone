# 3.3 Imports
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale

# 3.4.1 Load the ski resort data
ski_data = pd.read_csv(r'C:\Users\jwhit\OneDrive\Documents\Data Science Course\data\ski_data_cleaned.csv')
ski_data.info()
print(ski_data.head())

# 3.4.2 Load state-wide summary data
state_summary = pd.read_csv(r'C:\Users\jwhit\OneDrive\Documents\Data Science Course\data\state_summary.csv')
print(state_summary.head())

# 3.5.1.1-6 State-wide data exploration
state_summary_newind = state_summary.set_index('state')
print(state_summary_newind.state_area_sq_miles.sort_values(ascending=False).head())
print(state_summary_newind.state_population.sort_values(ascending=False).head())
print(state_summary_newind.resorts_per_state.sort_values(ascending=False).head())
print(state_summary_newind.state_total_skiable_area_ac.sort_values(ascending=False).head())
print(state_summary_newind.state_total_nightskiing_ac.sort_values(ascending=False).head())
print(state_summary_newind.state_total_days_open.sort_values(ascending=False).head())

# 3.5.2 Resort density
state_summary['resorts_per_100kcapita'] = 100_000 * state_summary.resorts_per_state / state_summary.state_population
state_summary['resorts_per_100ksq_mile'] = 100_000 * state_summary.resorts_per_state / state_summary.state_area_sq_miles
state_summary.drop(columns=['state_population', 'state_area_sq_miles'], inplace=True)
print(state_summary.head())

state_summary.resorts_per_100kcapita.hist(bins=30)
plt.xlabel('Number of resorts per 100k population')
plt.ylabel('count')
plt.show()

state_summary.resorts_per_100ksq_mile.hist(bins=30)
plt.xlabel('Number of resorts per 100k square miles')
plt.ylabel('count')
plt.show()

# 3.5.2.1 States resort density
print(state_summary.set_index('state').resorts_per_100kcapita.sort_values(ascending=False).head())
print(state_summary.set_index('state').resorts_per_100ksq_mile.sort_values(ascending=False).head())

# 3.5.3.1 Scale the data
state_summary_scale = state_summary.set_index('state')
state_summary_index = state_summary_scale.index
state_summary_columns = state_summary_scale.columns
state_summary_scale = scale(state_summary_scale)

state_summary_scaled_df = pd.DataFrame(state_summary_scale, columns=state_summary_columns)
print(state_summary_scaled_df.head())

# 3.5.3.1.1 Verifying scale
print(state_summary_scaled_df.mean())
print(state_summary_scaled_df.std())
print(state_summary_scaled_df.std(ddof=0))

# 3.5.3.2 Calculate the PCA transformation
state_pca = PCA().fit(state_summary_scale)
plt.subplots(figsize=(10, 6))
plt.plot(state_pca.explained_variance_ratio_.cumsum())
plt.xlabel('Component #')
plt.ylabel('Cumulative ratio variance')
plt.title('Cumulative variance ratio explained by PCA components for state/resort summary statistics')
plt.show()

state_pca_x = state_pca.transform(state_summary_scale)
x = state_pca_x[:, 0]
y = state_pca_x[:, 1]
state = state_summary_index
pc_var = 100 * state_pca.explained_variance_ratio_.cumsum()[1]
plt.subplots(figsize=(10, 8))
plt.scatter(x=x, y=y)
plt.xlabel('First component')
plt.ylabel('Second component')
plt.title(f'Ski states summary PCA, {pc_var:.1f}% variance explained')
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))
plt.show()

# 3.5.3.3 Average ticket price by state
state_avg_price = ski_data.groupby('state')['AdultWeekend'].mean()
print(state_avg_price.head())

state_avg_price.hist(bins=30)
plt.title('Distribution of state averaged prices')
plt.xlabel('Mean state adult weekend ticket price')
plt.ylabel('count')
plt.show()

# 3.5.3.4 Adding average ticket price to scatter plot
pca_df = pd.DataFrame({'PC1': state_pca_x[:, 0], 'PC2': state_pca_x[:, 1]}, index=state_summary_index)
pca_df = pd.concat([pca_df, state_avg_price], axis=1)
pca_df['Quartile'] = pd.qcut(pca_df.AdultWeekend, q=4, precision=1)
print(pca_df.head())
pca_df[pca_df.isnull().any(axis=1)]
pca_df['AdultWeekend'].fillna(pca_df.AdultWeekend.mean(), inplace=True)
pca_df['Quartile'] = pca_df['Quartile'].cat.add_categories('NA')
pca_df['Quartile'].fillna('NA', inplace=True)
print(pca_df.loc['Rhode Island'])

x = pca_df.PC1
y = pca_df.PC2
price = pca_df.AdultWeekend
quartiles = pca_df.Quartile
state = pca_df.index
pc_var = 100 * state_pca.explained_variance_ratio_.cumsum()[1]
fig, ax = plt.subplots(figsize=(10, 8))
for q in quartiles.cat.categories:
    im = quartiles == q
    ax.scatter(x=x[im], y=y[im], s=price[im], label=q)
ax.set_xlabel('First component')
ax.set_ylabel('Second component')
plt.legend()
ax.set_title(f'Ski states summary PCA, {pc_var:.1f}% variance explained')
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))
plt.show()

# Seaborn scatter plot
plt.subplots(figsize=(12, 10))
scatter_plot = sns.scatterplot(x='PC1', y='PC2', size='AdultWeekend', hue='Quartile', 
                hue_order=pca_df.Quartile.cat.categories, data=pca_df)

# Annotate the state labels
for s, x_val, y_val in zip(state, pca_df['PC1'], pca_df['PC2']):
    scatter_plot.annotate(s, (x_val, y_val))

plt.title(f'Ski states summary PCA, {pc_var:.1f}% variance explained')
plt.show()

# 3.5.5.1 Feature engineering
ski_data = ski_data.merge(state_summary, how='left', on='state')
print(ski_data.head().T)

ski_data['resort_skiable_area_ac_state_ratio'] = ski_data.SkiableTerrain_ac / ski_data.state_total_skiable_area_ac
ski_data['resort_days_open_state_ratio'] = ski_data.daysOpenLastYear / ski_data.state_total_days_open
ski_data['resort_terrain_park_state_ratio'] = ski_data.TerrainParks / ski_data.state_total_terrain_parks
ski_data['resort_night_skiing_state_ratio'] = ski_data.NightSkiing_ac / ski_data.state_total_nightskiing_ac
ski_data.drop(columns=['state_total_skiable_area_ac', 'state_total_days_open', 
                       'state_total_terrain_parks', 'state_total_nightskiing_ac'], inplace=True)

# 3.5.5.2 Correlation heatmap
plt.subplots(figsize=(12, 10))
sns.heatmap(ski_data.corr(numeric_only=True))
plt.show()

# 3.5.5.3 Scatterplots against ticket price
def scatterplots(columns, ncol=None, figsize=(15, 8)):
    if ncol is None:
        ncol = len(columns)
    nrow = int(np.ceil(len(columns) / ncol))
    fig, axes = plt.subplots(nrow, ncol, figsize=figsize, squeeze=False)
    fig.subplots_adjust(wspace=0.5, hspace=0.6)
    for i, col in enumerate(columns):
        ax = axes.flatten()[i]
        ax.scatter(x=col, y='AdultWeekend', data=ski_data, alpha=0.5)
        ax.set(xlabel=col, ylabel='Ticket price')
    nsubplots = nrow * ncol    
    for empty in range(i + 1, nsubplots):
        axes.flatten()[empty].set_visible(False)

features = [col for col in ski_data.columns if col not in ['Name', 'Region', 'state', 'AdultWeekend']]
scatterplots(features, ncol=4, figsize=(15, 15))

ski_data['total_chairs_runs_ratio'] = ski_data.total_chairs / ski_data.Runs
ski_data['total_chairs_skiable_ratio'] = ski_data.total_chairs / ski_data.SkiableTerrain_ac
ski_data['fastQuads_runs_ratio'] = ski_data.fastQuads / ski_data.Runs
ski_data['fastQuads_skiable_ratio'] = ski_data.fastQuads / ski_data.SkiableTerrain_ac

scatterplots(['total_chairs_runs_ratio', 'total_chairs_skiable_ratio', 
              'fastQuads_runs_ratio', 'fastQuads_skiable_ratio'], ncol=2)

# Save the data
datapath = r'C:\Users\jwhit\OneDrive\Documents\Data Science Course\data'
if not os.path.exists(datapath):
    os.makedirs(datapath)

def save_file(data, filename, datapath):
    filepath = os.path.join(datapath, filename)
    data.to_csv(filepath, index=False)

save_file(ski_data, 'ski_data_step3_features.csv', datapath)

3.6 SUMMARY
1.) What insights did you gain?
A.) Found that there were differences in average ticket prices among states, which indicates that state-related features could be significant in predicting ticket prices.
Identified potentially useful features such as SkiableTerrain_ac, daysOpenLastYear, TerrainParks, and NightSkiing_ac, and created derived features like resort_skiable_area_ac_state_ratio and resort_days_open_state_ratio.
2.) What does the resort level dataframe tell you?
A.) The resort-level dataframe provides detailed information about individual ski resorts and analyzes various aspects of each resort. The resort-level dataframe gives us
detailed resort features, competitive advantage, correlations between different features, price patterns, and state-level comparisons.
3.) How does dimensionality reduction help visualise data better?
A.) Principal Component Analysis (PCA) was used to reduce the dimensionality of the data, which helped in visualizing the relationships between states and identifying patterns that might not be obvious in the high-dimensional space.
It also indicated that a significant portion of the variance can be captured in a lower-dimensional space.
4.) What features could you extract from the data?
A.) State summary data, such as resorts_per_100kcapita and resorts_per_100ksq_mile. Resort features, such as SkiableTerrain_ac, daysOpenLastYear, TerrainParks, and NightSkiing_ac.
Derived ratios, such as resort_skiable_area_ac_state_ratio and resort_days_open_state_ratio.
5.) What correlations did you see in all features and their relationship with target variable?
A.) There is a positive correlation between the total skiable area and the AdultWeekend ticket price. Resorts with larger skiable areas tend to charge higher ticket prices.
The ratio of total chairs to the number of runs has a positive correlation with ticket prices. Resorts with more lifts per run may offer better access and convenience, justifying higher prices.
The number of terrain parks shows a weak correlation with ticket prices. While terrain parks can be an attractive feature, they do not appear to significantly influence ticket prices.
