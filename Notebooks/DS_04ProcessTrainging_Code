# 4.3 Imports
import pandas as pd
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import __version__ as sklearn_version
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_regression
import datetime
# Save function
def save_file(data, filename, datapath):
    filepath = os.path.join(datapath, filename)
    with open(filepath, 'wb') as file:
        pickle.dump(data, file)
# 4.4 Load Data

ski_data = pd.read_csv(r'C:\Users\jwhit\OneDrive\Documents\Data Science Course\data\ski_data_step3_features.csv')
ski_data.head().T

# 4.5 Extract Big Mountain Data

# Separate Big Mountain Resort data
big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']
big_mountain.T

# Display the shape of the original data
print(ski_data.shape)

# Remove Big Mountain Resort data from the dataset
ski_data = ski_data[ski_data.Name != 'Big Mountain Resort']

# Display the shape of the dataset after removing Big Mountain Resort
print(ski_data.shape)

# 4.6 Train/Split Test

# Split the data into train and test sets
X = ski_data.drop(columns=['AdultWeekend'])
y = ski_data['AdultWeekend']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Save the 'Name', 'state', and 'Region' columns from the train/test data into names_train and names_test
names_list = ['Name', 'state', 'Region']
names_train = X_train[names_list]
names_test = X_test[names_list]
X_train.drop(columns=names_list, inplace=True)
X_test.drop(columns=names_list, inplace=True)
X_train.shape, X_test.shape

# Check the `dtypes` attribute of `X_train` to verify all features are numeric
X_train.dtypes

# Repeat this check for the test split in `X_test`
X_test.dtypes

# Handle NaN values in target variables
y_train = y_train.fillna(y_train.median())
y_test = y_test.fillna(y_test.median())
# 4.7 Initial Not-Even-A-Model

# Calculate the mean of `y_train`
train_mean = y_train.mean()
print(train_mean)

# Fit the dummy regressor on the training data
dumb_reg = DummyRegressor(strategy='mean')
dumb_reg.fit(X_train, y_train)
print(dumb_reg.constant_)

# 4.7.1 Metrics

# Calculate the R^2 as defined above
def r_squared(y, ypred):
    """R-squared score.
    
    Calculate the R-squared, or coefficient of determination, of the input.
    
    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    ybar = np.sum(y) / len(y)  # yes, we could use np.mean(y)
    sum_sq_tot = np.sum((y - ybar)**2)  # total sum of squares error
    sum_sq_res = np.sum((y - ypred)**2)  # residual sum of squares error
    R2 = 1.0 - sum_sq_res / sum_sq_tot
    return R2
# Make predictions using the mean value as the prediction
y_tr_pred_ = train_mean * np.ones(len(y_train))
print(y_tr_pred_[:5])

# Make predictions using the dummy regressor
y_tr_pred = dumb_reg.predict(X_train)
print(y_tr_pred[:5])

# Calculate R-squared for training data
print(r_squared(y_train, y_tr_pred))

# Make predictions for test data
y_te_pred = train_mean * np.ones(len(y_test))

# Calculate R-squared for test data
print(r_squared(y_test, y_te_pred))

# 4.7.1.2 Mean Absolute Error

# Calculate the MAE as defined above
def mae(y, ypred):
    """Mean absolute error.
    
    Calculate the mean absolute error of the arguments

    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    abs_error = np.abs(y - ypred)
    mae = np.mean(abs_error)
    return mae

print(mae(y_train, y_tr_pred))
print(mae(y_test, y_te_pred))

# 4.7.1.3 Mean Squared Error

# Calculate the MSE as defined above
def mse(y, ypred):
    """Mean square error.
    
    Calculate the mean square error of the arguments

    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    sq_error = (y - ypred)**2
    mse = np.mean(sq_error)
    return mse

print(mse(y_train, y_tr_pred))
print(mse(y_test, y_te_pred))

# Root Mean Squared Error
print(np.sqrt([mse(y_train, y_tr_pred), mse(y_test, y_te_pred)]))

# 4.7.2 sklearn metrics

# R-squared
print(r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred))

# Mean Absolute Error
print(mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred))

# Mean Squared Error
print(mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred))

# Train set - sklearn R-squared
print(r2_score(y_train, y_tr_pred), r2_score(y_tr_pred, y_train))

# Test set - sklearn R-squared
print(r2_score(y_test, y_te_pred), r2_score(y_te_pred, y_test))

# Train set - using our homebrew function R-squared
print(r_squared(y_train, y_tr_pred), r_squared(y_tr_pred, y_train))

# Test set - using our homebrew function R-squared
print(r_squared(y_test, y_te_pred), r_squared(y_te_pred, y_test))

# 4.8 Initial Models
# 4.8.1 Imputing missing feature (predictor) values

# These are the values we'll use to fill in any missing values
X_defaults_median = X_train.median()
X_defaults_median

# 4.8.1.1.2 Apply the imputation to both train and test splits

# Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use

# Assign the results to `X_tr` and `X_te`, respectively
X_tr = X_train.fillna(X_defaults_median)
X_te = X_test.fillna(X_defaults_median)

# 4.8.1.1.3 Scale the data

# Call the StandardScaler`s fit method on `X_tr` to fit the scaler
# then use it's `transform()` method to apply the scaling to both the train and test split
# data (`X_tr` and `X_te`), naming the results `X_tr_scaled` and `X_te_scaled`, respectively
scaler = StandardScaler()
scaler.fit(X_tr)
X_tr_scaled = scaler.transform(X_tr)
X_te_scaled = scaler.transform(X_te)

# 4.8.1.1.4 Train the model on the train split

lm = LinearRegression().fit(X_tr_scaled, y_train)

# 4.8.1.1.5 Make predictions using the model on both train and test splits

# Call the `predict()` method of the model (`lm`) on both the (scaled) train and test data
# Assign the predictions to `y_tr_pred` and `y_te_pred`, respectively
y_tr_pred = lm.predict(X_tr_scaled)
y_te_pred = lm.predict(X_te_scaled)

# 4.8.1.1.6 Assess model performance

# r^2 - train, test
median_r2 = r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
print(median_r2)

# Refining The Linear Model
pipe = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(),
    SelectKBest(f_regression),
    LinearRegression()
)
# Calculate the mean absolute error scores using `sklearn`'s `mean_absolute_error` function
# as we did above for R^2
# MAE - train, test
median_mae = mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
print(median_mae)

# Calculate the mean squared error scores using `sklearn`'s `mean_squared_error`
# MSE - train, test
median_mse = mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
print(median_mse)

# 4.8.1.2 Impute missing values with the mean
# 4.8.1.2.1 Learn the values to impute from the train set

# Calculate mean values for imputing missing values
X_defaults_mean = X_train.mean()
print(X_defaults_mean)

# 4.8.1.2.2 Apply the imputation to both train and test splits

X_tr = X_train.fillna(X_defaults_mean)
X_te = X_test.fillna(X_defaults_mean)

# 4.8.1.2.3 Scale the data

scaler = StandardScaler()
scaler.fit(X_tr)
X_tr_scaled = scaler.transform(X_tr)
X_te_scaled = scaler.transform(X_te)

# 4.8.1.2.4 Train the model on the train split

lm = LinearRegression().fit(X_tr_scaled, y_train)

# 4.8.1.2.5 Make predictions using the model on both train and test splits

y_tr_pred = lm.predict(X_tr_scaled)
y_te_pred = lm.predict(X_te_scaled)

# 4.8.1.2.6 Assess model performance

print(r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred))

print(mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred))

print(mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred))

# 4.8.2 Pipelines
# 4.8.2.1 Define the pipeline

pipe = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(), 
    LinearRegression()
)
print(type(pipe))
print(hasattr(pipe, 'fit'), hasattr(pipe, 'predict'))

# 4.8.2.2 Fit the pipeline

# Call the pipe's `fit()` method with `X_train` and `y_train` as arguments
pipe.fit(X_train, y_train)

# 4.8.2.3 Make predictions on the train and test sets

y_tr_pred = pipe.predict(X_train)
y_te_pred = pipe.predict(X_test)

# 4.8.2.4 Assess performance

print(r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred))
print(median_r2)

print(mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred))
print(median_mae)

print(mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred))
print(median_mse)

# 4.9 Refining The Linear Model

# 4.9.1 Define the pipeline

# Add `SelectKBest` as a step in the pipeline between `StandardScaler()` and `LinearRegression()`
# Don't forget to tell it to use `f_regression` as its score function
pipe = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(),
    SelectKBest(f_regression),
    LinearRegression()
)
# 4.9.2 Fit the pipeline

pipe.fit(X_train, y_train)

# 4.9.3 Assess performance on the train and test set

y_tr_pred = pipe.predict(X_train)
y_te_pred = pipe.predict(X_test)
print(r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred))

print(mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred))

# 4.9.4 Define a new pipeline to select a different number of features

# Modify the `SelectKBest` step to use a value of 15 for k
pipe15 = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(),
    SelectKBest(f_regression, k=15),
    LinearRegression()
)
# 4.9.5 Fit the pipeline

pipe15.fit(X_train, y_train)

# 4.9.6 Assess performance on train and test data

y_tr_pred = pipe15.predict(X_train)
y_te_pred = pipe15.predict(X_test)
print(r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred))

print(mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred))

# 4.9.7 Assessing performance using cross-validation

cv_results = cross_validate(pipe15, X_train, y_train, cv=5)
cv_scores = cv_results['test_score']
print(cv_scores)

print(np.mean(cv_scores), np.std(cv_scores))
print(np.round((np.mean(cv_scores) - 2 * np.std(cv_scores), np.mean(cv_scores) + 2 * np.std(cv_scores)), 2))

# 4.9.8 Hyperparameter search using GridSearchCV

# Call `pipe`'s `get_params()` method to get a dict of available parameters and print their names
# using dict's `keys()` method
print(pipe.get_params().keys())

k = [k+1 for k in range(len(X_train.columns))]
grid_params = {'selectkbest__k': k}

lr_grid_cv = GridSearchCV(pipe, param_grid=grid_params, cv=5, n_jobs=-1)
lr_grid_cv.fit(X_train, y_train)

score_mean = lr_grid_cv.cv_results_['mean_test_score']
score_std = lr_grid_cv.cv_results_['std_test_score']
cv_k = [k for k in lr_grid_cv.cv_results_['param_selectkbest__k']]

# Print the `best_params_` attribute of `lr_grid_cv`
print(lr_grid_cv.best_params_)

# Assign the value of k from the above dict of `best_params_` and assign it to `best_k`
best_k = lr_grid_cv.best_params_['selectkbest__k']
plt.subplots(figsize=(10, 5))
plt.errorbar(cv_k, score_mean, yerr=score_std)
plt.axvline(x=best_k, c='r', ls='--', alpha=.5)
plt.xlabel('k')
plt.ylabel('CV score (r-squared)')
plt.title('Pipeline mean CV score (error bars +/- 1sd)')
plt.show()

selected = lr_grid_cv.best_estimator_.named_steps.selectkbest.get_support()

# Get the linear model coefficients from the `coef_` attribute and store in `coefs`,
# get the matching feature names from the column names of the dataframe,
# and display the results as a pandas Series with `coefs` as the values and `features` as the index,
# sorting the values in descending order
coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_
features = X_train.columns[selected]
print(pd.Series(coefs, index=features).sort_values(ascending=False))

# 4.10 Random Forest Model

# 4.10.1 Define the pipeline

# Define a pipeline comprising the steps:
# SimpleImputer() with a strategy of 'median'
# StandardScaler(),
# and then RandomForestRegressor() with a random state of 47
RF_pipe = make_pipeline(
    SimpleImputer(strategy='median'),
    StandardScaler(),
    RandomForestRegressor(random_state=47)
)

# 4.10.2 Fit and assess performance using cross-validation

# Call `cross_validate` to estimate the pipeline's performance.
# Pass it the random forest pipe object, `X_train` and `y_train`,
# and get it to use 5-fold cross-validation
rf_default_cv_results = cross_validate(RF_pipe, X_train, y_train, cv=5)
rf_cv_scores = rf_default_cv_results['test_score']
print(rf_cv_scores)

print(np.mean(rf_cv_scores), np.std(rf_cv_scores))

# 4.10.3 Hyperparameter search using GridSearchCV

n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]
grid_params = {
        'randomforestregressor__n_estimators': n_est,
        'standardscaler': [StandardScaler(), None],
        'simpleimputer__strategy': ['mean', 'median']
}
print(grid_params)

# Call `GridSearchCV` with the random forest pipeline, passing in the above `grid_params`
# dict for parameters to evaluate, 5-fold cross-validation, and all available CPU cores (if desired)
rf_grid_cv = GridSearchCV(RF_pipe, param_grid=grid_params, cv=5, n_jobs=-1)

# Now call the `GridSearchCV`'s `fit()` method with `X_train` and `y_train` as arguments
# to actually start the grid search. This may take a minute or two.
rf_grid_cv.fit(X_train, y_train)

# Print the best params (`best_params_` attribute) from the grid search
print(rf_grid_cv.best_params_)

rf_best_cv_results = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, cv=5)
rf_best_scores = rf_best_cv_results['test_score']
print(rf_best_scores)

print(np.mean(rf_best_scores), np.std(rf_best_scores))

# Plot a barplot of the random forest's feature importances,
# assigning the `feature_importances_` attribute of 
# `rf_grid_cv.best_estimator_.named_steps.randomforestregressor` to the name `imps` to then
# create a pandas Series object of the feature importances, with the index given by the
# training data column names, sorting the values in descending order
plt.subplots(figsize=(10, 5))
imps = rf_grid_cv.best_estimator_.named_steps.randomforestregressor.feature_importances_
rf_feat_imps = pd.Series(imps, index=X_train.columns).sort_values(ascending=False)
rf_feat_imps.plot(kind='bar')
plt.xlabel('features')
plt.ylabel('importance')
plt.title('Best random forest regressor feature importances')
plt.show()

# 4.11 Final Model Selection

# 4.11.1 Linear regression model performance

# 'neg_mean_absolute_error' uses the (negative of) the mean absolute error
lr_neg_mae = cross_validate(lr_grid_cv.best_estimator_, X_train, y_train, 
                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)
lr_mae_mean = np.mean(-1 * lr_neg_mae['test_score'])
lr_mae_std = np.std(-1 * lr_neg_mae['test_score'])
print(lr_mae_mean, lr_mae_std)

print(mean_absolute_error(y_test, lr_grid_cv.best_estimator_.predict(X_test)))

print(mean_absolute_error(y_test, lr_grid_cv.best_estimator_.predict(X_test)))

# 4.11.2 Random forest regression model performance

rf_neg_mae = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, 
                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)
rf_mae_mean = np.mean(-1 * rf_neg_mae['test_score'])
rf_mae_std = np.std(-1 * rf_neg_mae['test_score'])
print(rf_mae_mean, rf_mae_std)

print(mean_absolute_error(y_test, rf_grid_cv.best_estimator_.predict(X_test)))

# 4.12 Data Quantity Assessment

fractions = [.2, .25, .3, .35, .4, .45, .5, .6, .75, .8, 1.0]
train_size, train_scores, test_scores = learning_curve(pipe, X_train, y_train, train_sizes=fractions)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)
plt.subplots(figsize=(10, 5))
plt.errorbar(train_size, test_scores_mean, yerr=test_scores_std)
plt.xlabel('Training set size')
plt.ylabel('CV scores')
plt.title('Cross-validation score as training set size increases')
plt.show()

# 4.13 Save Best Model Object
best_model = rf_grid_cv.best_estimator_
best_model.version = '1.0'
best_model.pandas_version = pd.__version__
best_model.numpy_version = np.__version__
best_model.sklearn_version = sklearn_version
best_model.X_columns = [col for col in X_train.columns]
best_model.build_datetime = datetime.datetime.now()



# Save the model
modelpath = 'C:/Users/jwhit/OneDrive/Documents/Data Science Course/data'
with open(os.path.join(modelpath, 'ski_resort_pricing_model.pkl'), 'wb') as file:
    pickle.dump(best_model, file)
